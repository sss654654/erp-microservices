# 06. buildspec.yml ì‘ì„± (CodePipeline ê°•ì  ê·¹ëŒ€í™”)

**ì†Œìš” ì‹œê°„**: 3ì‹œê°„  
**ëª©í‘œ**: 7ê°€ì§€ CodePipeline ê°•ì ì„ ëª¨ë‘ êµ¬í˜„

---

##  06ë‹¨ê³„ì—ì„œ êµ¬í˜„í•  ê¸°ëŠ¥

### CGVì™€ ì°¨ë³„í™” (4ê°€ì§€)

| ê¸°ëŠ¥ | CGV (GitLab CI) | ERP (CodePipeline) | êµ¬í˜„ ìœ„ì¹˜ |
|------|----------------|-------------------|----------|
| Secret ê´€ë¦¬ | GitLab Variables |  AWS Secrets Manager | 01ë‹¨ê³„ ì™„ë£Œ |
| ë‹¨ì¼ íŒŒì´í”„ë¼ì¸ | ì„œë¹„ìŠ¤ë³„ ë…ë¦½ |  Helm Chart í†µí•© | 05ë‹¨ê³„ ì™„ë£Œ |
| **ì„¤ì • ê´€ë¦¬** | **í•˜ë“œì½”ë”©** | ** Parameter Store** | **Step 1** |
| **ë¡œê·¸ ê´€ë¦¬** | **GitLab Logs** | ** CloudWatch Logs** | **Step 2** |
| **íŠ¸ë ˆì´ì‹±** | **ì—†ìŒ** | ** X-Ray í†µí•©** | **Step 3** |
| **ì´ë¯¸ì§€ ìŠ¤ìº” + ë³€ê²½ ê°ì§€** | **ìˆ˜ë™ + ì „ì²´ ë¹Œë“œ** | ** ECR ìŠ¤ìº” + Git diff** | **Step 4** |

---

## Step 1: Parameter Store í™œìš© (20ë¶„)

### 1-1. ì™œ í•„ìš”í•œê°€?

**í˜„ì¬ ë¬¸ì œ:**
```yaml
# buildspec.yml í•˜ë“œì½”ë”©
env:
  variables:
    AWS_ACCOUNT_ID: "806332783810"      #  ê³„ì • ë³€ê²½ ì‹œ ìˆ˜ì • í•„ìš”
    AWS_REGION: "ap-northeast-2"        #  í™˜ê²½ë³„ ë¶„ë¦¬ ë¶ˆê°€
    EKS_CLUSTER_NAME: "erp-dev"         #  Gitì— ë…¸ì¶œ
```

**Parameter Store ì‚¬ìš© ì‹œ:**
```yaml
env:
  parameter-store:
    AWS_ACCOUNT_ID: /erp/dev/account-id              #  ì¤‘ì•™ ê´€ë¦¬
    AWS_REGION: /erp/dev/region                      #  í™˜ê²½ë³„ ë¶„ë¦¬
    EKS_CLUSTER_NAME: /erp/dev/eks/cluster-name      #  Gitì— ì•ˆì „
```

### 1-2. Terraformìœ¼ë¡œ Parameter ìƒì„±

```bash
cd infrastructure/terraform/dev/erp-dev-ParameterStore

terraform init
terraform apply -auto-approve
```

**ìƒì„±ë˜ëŠ” 6ê°œ Parameter:**
```
/erp/dev/account-id              # data.aws_caller_identityë¡œ ìë™
/erp/dev/region                  # ap-northeast-2
/erp/dev/eks/cluster-name        # remote_state.eksë¡œ ìë™
/erp/dev/ecr/repository-prefix   # erp
/erp/dev/project-name            # erp
/erp/dev/environment             # dev
```

**í™•ì¸:**
```bash
terraform output
```

### 1-3. CodeBuild Role ê¶Œí•œ í™•ì¸

**ì´ë¯¸ 02_TERRAFORM.mdì—ì„œ ì™„ë£Œ:**
```bash
aws iam list-role-policies --role-name erp-dev-codebuild-role --region ap-northeast-2
# codebuild-ssm-policy 
```

---

## Step 2: CloudWatch Logs ì¤‘ì•™ ì§‘ì¤‘ (30ë¶„)

### 2-1. ê°œë… ì´í•´: CloudWatch Logsê°€ ë­”ê°€ìš”?

**ì‰½ê²Œ ì„¤ëª…í•˜ë©´:**
- PodëŠ” ì»¨í…Œì´ë„ˆ ì•ˆì—ì„œ ì‹¤í–‰ë˜ëŠ” í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤
- í”„ë¡œê·¸ë¨ì´ ì‹¤í–‰ë˜ë©´ ë¡œê·¸(ê¸°ë¡)ê°€ ìƒì„±ë©ë‹ˆë‹¤
- ì´ ë¡œê·¸ë¥¼ ì–´ë””ì— ì €ì¥í• ê¹Œìš”?

**í˜„ì¬ ìƒí™© (ë¬¸ì œ):**
```
Pod ì•ˆì—ë§Œ ë¡œê·¸ ì €ì¥
    â†“
Pod ì¬ì‹œì‘ â†’ ë¡œê·¸ ì‚¬ë¼ì§ 
Pod ì—¬ëŸ¬ ê°œ â†’ ê°ê° í™•ì¸í•´ì•¼ í•¨ 
```

**CloudWatch Logs ì‚¬ìš© (í•´ê²°):**
```
Pod ë¡œê·¸ â†’ Fluent Bit â†’ CloudWatch Logs (AWS ì €ì¥ì†Œ)
    â†“
Pod ì¬ì‹œì‘í•´ë„ ë¡œê·¸ ìœ ì§€ 
ëª¨ë“  Pod ë¡œê·¸ í•œ ê³³ì—ì„œ ê²€ìƒ‰ 
```

### 2-2. ì‹¤ì œ ì˜ˆì‹œë¡œ ì´í•´í•˜ê¸°

**ì‹œë‚˜ë¦¬ì˜¤: ì—ëŸ¬ ë°œìƒ ì‹œ**

**Before (CloudWatch ì—†ìŒ):**
```bash
# 1. ì–´ëŠ Podì—ì„œ ì—ëŸ¬ ë‚¬ëŠ”ì§€ ëª¨ë¦„
kubectl get pods -n erp-dev
# approval-request-service-abc123
# approval-request-service-def456

# 2. ê° Pod ë¡œê·¸ ì¼ì¼ì´ í™•ì¸
kubectl logs approval-request-service-abc123 -n erp-dev
kubectl logs approval-request-service-def456 -n erp-dev

# 3. Pod ì¬ì‹œì‘ë˜ë©´ ë¡œê·¸ ì‚¬ë¼ì§
kubectl delete pod approval-request-service-abc123 -n erp-dev
# ë¡œê·¸ ì˜êµ¬ ì†Œì‹¤ 
```

**After (CloudWatch ì‚¬ìš©):**
```bash
# 1. AWS Console â†’ CloudWatch Logs
# 2. /aws/eks/erp-dev/application í´ë¦­
# 3. ê²€ìƒ‰ì°½ì— "ERROR" ì…ë ¥
# 4. ëª¨ë“  Podì˜ ì—ëŸ¬ ë¡œê·¸ê°€ í•œ ë²ˆì— ê²€ìƒ‰ë¨ 
# 5. Pod ì¬ì‹œì‘í•´ë„ ë¡œê·¸ ìœ ì§€ 
```

### 2-3. êµ¬ì„± ìš”ì†Œ 3ê°€ì§€

#### â‘  IAM ê¶Œí•œ (EKS Nodeê°€ CloudWatchì— ì“¸ ìˆ˜ ìˆê²Œ)

**ì™œ í•„ìš”í•œê°€?**
- EKS Node(ì„œë²„)ê°€ CloudWatchì— ë¡œê·¸ë¥¼ ë³´ë‚´ë ¤ë©´ ê¶Œí•œ í•„ìš”
- ì§‘ì— íƒë°° ë³´ë‚´ë ¤ë©´ ì£¼ì†Œ ì•Œì•„ì•¼ í•˜ëŠ” ê²ƒê³¼ ê°™ìŒ

**ì„¤ì • ë‚´ìš©:**
```hcl
# infrastructure/terraform/dev/erp-dev-IAM/eks-node-role/eks-node-role.tf
resource "aws_iam_role_policy" "eks_node_cloudwatch_logs" {
  role = aws_iam_role.eks_node.name
  name = "eks-node-cloudwatch-logs-policy"

  policy = jsonencode({
    Statement = [{
      Effect = "Allow"
      Action = [
        "logs:CreateLogGroup",      # ë¡œê·¸ ê·¸ë£¹ ë§Œë“¤ê¸°
        "logs:CreateLogStream",     # ë¡œê·¸ ìŠ¤íŠ¸ë¦¼ ë§Œë“¤ê¸°
        "logs:PutLogEvents",        # ë¡œê·¸ ì“°ê¸°
        "logs:DescribeLogStreams"   # ë¡œê·¸ ìŠ¤íŠ¸ë¦¼ í™•ì¸
      ]
      Resource = "arn:aws:logs:ap-northeast-2:806332783810:log-group:/aws/eks/erp-dev/*"
    }]
  })
}
```

**í™•ì¸ ë°©ë²•:**
```bash
# IAM ê¶Œí•œ í™•ì¸
aws iam list-role-policies --role-name erp-dev-eks-node-role --region ap-northeast-2

# ì¶œë ¥ì— "eks-node-cloudwatch-logs-policy" ìˆìœ¼ë©´ ì„±ê³µ 
```

#### â‘¡ Fluent Bit (ë¡œê·¸ ìˆ˜ì§‘ê¸°)

**ì™œ í•„ìš”í•œê°€?**
- Pod ë¡œê·¸ë¥¼ ìë™ìœ¼ë¡œ CloudWatchë¡œ ë³´ë‚´ì£¼ëŠ” í”„ë¡œê·¸ë¨
- ìš°ì²´ë¶€ê°€ í¸ì§€ë¥¼ ìˆ˜ê±°í•´ì„œ ìš°ì²´êµ­ìœ¼ë¡œ ë³´ë‚´ëŠ” ê²ƒê³¼ ê°™ìŒ

**ë™ì‘ ë°©ì‹:**
```
1. Fluent Bitì´ ê° Nodeì— 1ê°œì”© ì‹¤í–‰ë¨ (DaemonSet)
2. í•´ë‹¹ Nodeì˜ ëª¨ë“  Pod ë¡œê·¸ë¥¼ ì½ìŒ
3. CloudWatch Logsë¡œ ì „ì†¡
```

**ì„¤ì • ë‚´ìš©:**
```yaml
# helm-chart/templates/fluent-bit.yaml
apiVersion: apps/v1
kind: DaemonSet  # ê° Nodeì— 1ê°œì”© ì‹¤í–‰
metadata:
  name: fluent-bit
  namespace: amazon-cloudwatch
spec:
  template:
    spec:
      containers:
      - name: fluent-bit
        image: public.ecr.aws/aws-observability/aws-for-fluent-bit:latest
        env:
        - name: AWS_REGION
          value: ap-northeast-2
        - name: CLUSTER_NAME
          value: erp-dev
        - name: LOG_GROUP_NAME
          value: /aws/eks/erp-dev/application
```

**í™•ì¸ ë°©ë²•:**
```bash
# Fluent Bit Pod í™•ì¸
kubectl get pods -n amazon-cloudwatch

# ì¶œë ¥ ì˜ˆì‹œ:
# NAME               READY   STATUS    RESTARTS   AGE
# fluent-bit-xxxxx   1/1     Running   0          10m
# fluent-bit-yyyyy   1/1     Running   0          10m
# â†’ Node 2ê°œì´ë¯€ë¡œ Pod 2ê°œ 

# Fluent Bit ë¡œê·¸ í™•ì¸ (ì •ìƒ ë™ì‘ ì—¬ë¶€)
kubectl logs -n amazon-cloudwatch -l app.kubernetes.io/name=fluent-bit --tail=20

# ì¶œë ¥ì— "Fluent Bit v2.x started" ìˆìœ¼ë©´ ì„±ê³µ 
```

#### â‘¢ CloudWatch Log Group (ë¡œê·¸ ì €ì¥ì†Œ)

**ì™œ í•„ìš”í•œê°€?**
- ë¡œê·¸ë¥¼ ì €ì¥í•  í´ë” ê°™ì€ ê²ƒ
- ì„œë¹„ìŠ¤ë³„ë¡œ í´ë”ë¥¼ ë‚˜ëˆ ì„œ ê´€ë¦¬

**êµ¬ì¡°:**
```
/aws/eks/erp-dev/application (Log Group)
â”œâ”€â”€ approval-request-service-abc123 (Log Stream)
â”‚   â””â”€â”€ 2025-12-28 16:00:00 [INFO] Started application
â”‚   â””â”€â”€ 2025-12-28 16:00:01 [ERROR] Connection failed
â”œâ”€â”€ approval-request-service-def456 (Log Stream)
â”‚   â””â”€â”€ 2025-12-28 16:00:00 [INFO] Started application
â””â”€â”€ notification-service-xyz789 (Log Stream)
    â””â”€â”€ 2025-12-28 16:00:00 [INFO] WebSocket connected
```

**í™•ì¸ ë°©ë²•:**
```bash
# CloudWatch Log Group í™•ì¸
aws logs describe-log-groups \
  --log-group-name-prefix /aws/eks/erp-dev \
  --region ap-northeast-2 \
  --query 'logGroups[*].logGroupName' \
  --output table

# ì¶œë ¥ ì˜ˆì‹œ:
# --------------------------------
# |   DescribeLogGroups          |
# +------------------------------+
# |  /aws/eks/erp-dev/application|
# +------------------------------+
#  Log Group ìƒì„±ë¨

# Log Stream í™•ì¸ (Podë³„ ë¡œê·¸)
aws logs describe-log-streams \
  --log-group-name /aws/eks/erp-dev/application \
  --region ap-northeast-2 \
  --max-items 5 \
  --query 'logStreams[*].logStreamName' \
  --output table

# ì¶œë ¥ ì˜ˆì‹œ:
# ------------------------------------------------
# |   DescribeLogStreams                         |
# +----------------------------------------------+
# |  approval-request-service-abc123             |
# |  approval-request-service-def456             |
# |  notification-service-xyz789                 |
# +----------------------------------------------+
#  Podë³„ Log Stream ìƒì„±ë¨
```

### 2-4. ì‹¤ì œ ë¡œê·¸ í™•ì¸í•˜ê¸°

#### ë°©ë²• 1: AWS CLI (í„°ë¯¸ë„)

```bash
# ìµœê·¼ 5ë¶„ ë¡œê·¸ í™•ì¸
aws logs tail /aws/eks/erp-dev/application --since 5m --region ap-northeast-2

# ì‹¤ì‹œê°„ ë¡œê·¸ ìŠ¤íŠ¸ë¦¬ë° (ê³„ì† ë³´ê¸°)
aws logs tail /aws/eks/erp-dev/application --follow --region ap-northeast-2

# íŠ¹ì • í‚¤ì›Œë“œ ê²€ìƒ‰ (ERRORë§Œ)
aws logs tail /aws/eks/erp-dev/application --since 1h --region ap-northeast-2 | grep ERROR
```

#### ë°©ë²• 2: AWS Console (ì›¹)

```
1. AWS Console ë¡œê·¸ì¸
2. CloudWatch ì„œë¹„ìŠ¤ í´ë¦­
3. ì™¼ìª½ ë©”ë‰´ â†’ Logs â†’ Log groups
4. /aws/eks/erp-dev/application í´ë¦­
5. Log streamsì—ì„œ Pod ì„ íƒ
6. ë¡œê·¸ í™•ì¸

ê²€ìƒ‰ ê¸°ëŠ¥:
- Filter events ì…ë ¥ì°½ì— "ERROR" ì…ë ¥
- ëª¨ë“  Podì˜ ì—ëŸ¬ ë¡œê·¸ê°€ í•œ ë²ˆì— ê²€ìƒ‰ë¨ 
```

#### ë°©ë²• 3: CloudWatch Insights (ê³ ê¸‰ ê²€ìƒ‰)

```
1. CloudWatch â†’ Logs â†’ Insights
2. Log group ì„ íƒ: /aws/eks/erp-dev/application
3. ì¿¼ë¦¬ ì…ë ¥:

# ìµœê·¼ 1ì‹œê°„ ì—ëŸ¬ ë¡œê·¸ ê°œìˆ˜
fields @timestamp, @message
| filter @message like /ERROR/
| stats count() by bin(5m)

# ì„œë¹„ìŠ¤ë³„ ì—ëŸ¬ ê°œìˆ˜
fields @timestamp, @message
| filter @message like /ERROR/
| parse @logStream /(?<service>[^-]+)-service/
| stats count() by service
```

### 2-5. ì™„ë£Œ í™•ì¸ ì²´í¬ë¦¬ìŠ¤íŠ¸

```bash
#  1. IAM ê¶Œí•œ í™•ì¸
aws iam list-role-policies --role-name erp-dev-eks-node-role --region ap-northeast-2 | grep cloudwatch

#  2. Fluent Bit Pod í™•ì¸
kubectl get pods -n amazon-cloudwatch
# 2ê°œ Pod Running í™•ì¸

#  3. CloudWatch Log Group í™•ì¸
aws logs describe-log-groups --log-group-name-prefix /aws/eks/erp-dev --region ap-northeast-2

#  4. Log Stream í™•ì¸ (Podë³„ ë¡œê·¸)
aws logs describe-log-streams --log-group-name /aws/eks/erp-dev/application --region ap-northeast-2 --max-items 5

#  5. ì‹¤ì œ ë¡œê·¸ í™•ì¸
aws logs tail /aws/eks/erp-dev/application --since 5m --region ap-northeast-2
```

### 2-6. ì™œ ì´ê²Œ ì¤‘ìš”í•œê°€? (ì‹¤ë¬´ ê´€ì )

**ì‹œë‚˜ë¦¬ì˜¤ 1: ìƒˆë²½ 3ì‹œ ì¥ì•  ë°œìƒ**
```
Before:
- ìƒˆë²½ì— Pod ì¬ì‹œì‘ë¨
- ì•„ì¹¨ì— ì¶œê·¼í•´ì„œ í™•ì¸í•˜ë ¤ë‹ˆ ë¡œê·¸ ì‚¬ë¼ì§ 
- ì›ì¸ íŒŒì•… ë¶ˆê°€

After:
- CloudWatchì— ëª¨ë“  ë¡œê·¸ ì €ì¥ë¨
- ì•„ì¹¨ì— ì¶œê·¼í•´ì„œ CloudWatch í™•ì¸
- ìƒˆë²½ 3ì‹œ ë¡œê·¸ ê·¸ëŒ€ë¡œ ë‚¨ì•„ìˆìŒ 
- ì›ì¸ íŒŒì•… ê°€ëŠ¥
```

**ì‹œë‚˜ë¦¬ì˜¤ 2: íŠ¹ì • ì—ëŸ¬ íŒ¨í„´ ì°¾ê¸°**
```
Before:
- 10ê°œ Pod ë¡œê·¸ë¥¼ ì¼ì¼ì´ í™•ì¸
- kubectl logs 10ë²ˆ ì‹¤í–‰
- ì‹œê°„ ë‚­ë¹„

After:
- CloudWatch Insights ì¿¼ë¦¬ 1ë²ˆ
- ëª¨ë“  Podì—ì„œ ì—ëŸ¬ íŒ¨í„´ ê²€ìƒ‰
- 1ë¶„ ì•ˆì— ì™„ë£Œ 
```

**ì‹œë‚˜ë¦¬ì˜¤ 3: ì•ŒëŒ ì„¤ì • (ì„ íƒ ì‚¬í•­)**
```
CloudWatch Logs â†’ CloudWatch Alarm
- "ERROR" í‚¤ì›Œë“œê°€ 10ë²ˆ ì´ìƒ ë‚˜ì˜¤ë©´
- Slack/Email ì•ŒëŒ ë°œì†¡
- ìë™ ëª¨ë‹ˆí„°ë§ 

ì°¸ê³ : ì•ŒëŒ ì„¤ì •ì€ í˜„ì¬ í”„ë¡œì íŠ¸ ë²”ìœ„ì— í¬í•¨ë˜ì§€ ì•ŠìŒ
```

---

### 2-7. Step 2 ì™„ë£Œ í™•ì¸ ë° ì •ë¦¬

#### ì™„ë£Œëœ ì‘ì—… ì²´í¬ë¦¬ìŠ¤íŠ¸

```bash
# 1. IAM ê¶Œí•œ í™•ì¸
aws iam list-role-policies --role-name erp-dev-eks-node-role --region ap-northeast-2 | grep cloudwatch
# ì¶œë ¥: eks-node-cloudwatch-logs-policy

# 2. Fluent Bit Pod í™•ì¸
kubectl get pods -n amazon-cloudwatch
# ì¶œë ¥: fluent-bit-xxxxx 2ê°œ Running

# 3. CloudWatch Log Group í™•ì¸
aws logs describe-log-groups --log-group-name /aws/eks/erp-dev/application --region ap-northeast-2
# ì¶œë ¥: logGroupName: /aws/eks/erp-dev/application

# 4. Log Stream í™•ì¸ (Podë³„ ë¡œê·¸)
aws logs describe-log-streams --log-group-name /aws/eks/erp-dev/application --region ap-northeast-2 --max-items 5
# ì¶œë ¥: Podë³„ Log Stream ëª©ë¡

# 5. ì‹¤ì œ ë¡œê·¸ í™•ì¸
aws logs tail /aws/eks/erp-dev/application --since 5m --region ap-northeast-2
# ì¶œë ¥: ìµœê·¼ 5ë¶„ê°„ì˜ ë¡œê·¸
```

#### í˜„ì¬ Log Group êµ¬ì¡°

```
AWS CloudWatch Logs
â”œâ”€â”€ /aws/eks/erp-dev/application (EKS Pod ë¡œê·¸)
â”‚   â”œâ”€â”€ fluentbit-kube...approval-request-service-xxx (Pod 1)
â”‚   â”œâ”€â”€ fluentbit-kube...approval-request-service-yyy (Pod 2)
â”‚   â”œâ”€â”€ fluentbit-kube...approval-processing-service-xxx (Pod 1)
â”‚   â”œâ”€â”€ fluentbit-kube...approval-processing-service-yyy (Pod 2)
â”‚   â”œâ”€â”€ fluentbit-kube...notification-service-xxx (Pod 1)
â”‚   â””â”€â”€ fluentbit-kube...notification-service-yyy (Pod 2)
â”‚
â”œâ”€â”€ /aws/lambda/erp-dev-employee-service (Lambda ë¡œê·¸)
â”‚   â””â”€â”€ Lambda ì‹¤í–‰ë§ˆë‹¤ ìë™ ìƒì„±
â”‚
â””â”€â”€ /aws/apigateway/erp-dev-api (API Gateway ë¡œê·¸, ì„ íƒ)
    â””â”€â”€ API ìš”ì²­ ë¡œê·¸
```

#### ë¡œê·¸ íë¦„ ì •ë¦¬

**EKS Pod ë¡œê·¸:**
```
1. Podê°€ stdout/stderrë¡œ ë¡œê·¸ ì¶œë ¥
   ì˜ˆ: System.out.println("Hello")

2. Kubernetesê°€ /var/log/containers/*.logì— ì €ì¥

3. Fluent Bit (DaemonSet)ì´ í•´ë‹¹ íŒŒì¼ ì½ìŒ
   - ê° Nodeì— 1ê°œì”© ì‹¤í–‰
   - í•´ë‹¹ Nodeì˜ ëª¨ë“  Pod ë¡œê·¸ ìˆ˜ì§‘

4. CloudWatch Logsë¡œ ì „ì†¡
   - Log Group: /aws/eks/erp-dev/application
   - Log Stream: Podë³„ë¡œ ìë™ ìƒì„±

5. API ìš”ì²­ ì‹œ ì‹¤ì‹œê°„ìœ¼ë¡œ ë¡œê·¸ ìŒ“ì„
   - Pod ì¬ì‹œì‘í•´ë„ ë¡œê·¸ ìœ ì§€
   - ì˜êµ¬ ë³´ê´€ (ë§Œë£Œ ì—†ìŒ)
```

**Lambda ë¡œê·¸:**
```
1. Lambda í•¨ìˆ˜ ì‹¤í–‰
2. CloudWatch Logs ìë™ ì „ì†¡ (ë‚´ì¥ ê¸°ëŠ¥)
3. Log Group: /aws/lambda/erp-dev-employee-service
4. Log Stream: ì‹¤í–‰ë§ˆë‹¤ ìë™ ìƒì„±
```

#### íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê¸°ë¡

**ë¬¸ì œ: ë¡œê·¸ê°€ ì•ˆ ìŒ“ì„**
```
ì¦ìƒ: ë§ˆì§€ë§‰ ì´ë²¤íŠ¸ ì‹œê°„ì´ 4ì‹œê°„ ì „ì— ë©ˆì¶¤

ì›ì¸: Fluent Bitì´ ì˜ëª»ëœ Log Groupì— ì“°ë ¤ê³  ì‹œë„
- ì‹œë„: /aws/eks/erp-dev (í‹€ë¦¼)
- ì •ìƒ: /aws/eks/erp-dev/application (ë§ìŒ)

í•´ê²°:
1. helm-chart/values-dev.yaml ìˆ˜ì •
   logGroupName: /aws/eks/erp-dev/application

2. Helm ì¬ë°°í¬
   helm upgrade --install erp-microservices helm-chart/ -f helm-chart/values-dev.yaml -n erp-dev

3. Fluent Bit ì¬ì‹œì‘
   kubectl rollout restart daemonset fluent-bit -n amazon-cloudwatch

4. í™•ì¸
   kubectl logs -n amazon-cloudwatch fluent-bit-xxxxx --tail=20
   ì¶œë ¥: "Created log stream..." ë©”ì‹œì§€ í™•ì¸
```

#### Step 2 ì™„ë£Œ!

CloudWatch Logs ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.

**ë‹¬ì„±í•œ ê²ƒ:**
- EKS Pod ë¡œê·¸ ì¤‘ì•™ ì§‘ì¤‘í™”
- Pod ì¬ì‹œì‘í•´ë„ ë¡œê·¸ ìœ ì§€
- ëª¨ë“  Pod ë¡œê·¸ í†µí•© ê²€ìƒ‰ ê°€ëŠ¥
- Lambda ë¡œê·¸ ìë™ ìˆ˜ì§‘

**ë‹¤ìŒ ë‹¨ê³„:**
- Step 3: X-Ray íŠ¸ë ˆì´ì‹± í†µí•©

---

## Step 3: X-Ray íŠ¸ë ˆì´ì‹± í†µí•© (40ë¶„)

### 3-1. ê°œë… ì´í•´: X-Rayê°€ ë­”ê°€ìš”?

**ì‰½ê²Œ ì„¤ëª…í•˜ë©´:**
- ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ëŠ” ì—¬ëŸ¬ ì„œë¹„ìŠ¤ê°€ ì„œë¡œ í˜¸ì¶œí•©ë‹ˆë‹¤
- ì‚¬ìš©ì ìš”ì²­ì´ ì–´ë–¤ ê²½ë¡œë¡œ í˜ëŸ¬ê°€ëŠ”ì§€ ì¶”ì í•˜ëŠ” ë„êµ¬

**ì˜ˆì‹œ: ê²°ì¬ ìš”ì²­ íë¦„**
```
ì‚¬ìš©ì â†’ API Gateway â†’ approval-request-service â†’ employee-service (Lambda) â†’ RDS
                              â†“
                       notification-service â†’ Redis
```

**ë¬¸ì œ:**
- ì–´ëŠ ì„œë¹„ìŠ¤ê°€ ëŠë¦°ì§€ ëª¨ë¦„
- ì—ëŸ¬ê°€ ì–´ë””ì„œ ë°œìƒí–ˆëŠ”ì§€ ëª¨ë¦„
- ë³‘ëª© ì§€ì ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ

**X-Ray ì‚¬ìš© ì‹œ:**
```
Service Map (ì‹œê°í™”):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API Gateway â”‚ â†’   â”‚ approval-req â”‚ â†’   â”‚ employee â”‚
â”‚   50ms      â”‚     â”‚    200ms     â”‚     â”‚  150ms   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ notification â”‚
                    â”‚    100ms     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â†’ approval-request-serviceê°€ ê°€ì¥ ëŠë¦¼ (200ms)
â†’ ì—¬ê¸°ë¥¼ ìµœì í™”í•˜ë©´ ì „ì²´ ì„±ëŠ¥ í–¥ìƒ 
```

### 3-2. ì‹¤ì œ ì˜ˆì‹œë¡œ ì´í•´í•˜ê¸°

**ì‹œë‚˜ë¦¬ì˜¤: ì‚¬ìš©ìê°€ "ê²°ì¬ ìš”ì²­" ë²„íŠ¼ í´ë¦­**

**Before (X-Ray ì—†ìŒ):**
```
ì‚¬ìš©ì: "ì™œ ì´ë ‡ê²Œ ëŠë ¤ìš”?"
ê°œë°œì: "ì–´ë””ê°€ ëŠë¦°ì§€ ëª¨ë¥´ê² ëŠ”ë°ìš”..."
â†’ ê° ì„œë¹„ìŠ¤ ë¡œê·¸ ì¼ì¼ì´ í™•ì¸
â†’ ì‹œê°„ ë‚­ë¹„
```

**After (X-Ray ì‚¬ìš©):**
```
X-Ray Service Map í™•ì¸:
- API Gateway: 50ms 
- approval-request: 200ms  (ëŠë¦¼!)
- employee (Lambda): 150ms 
- notification: 100ms 

â†’ approval-request-serviceê°€ ë¬¸ì œ
â†’ ì½”ë“œ í™•ì¸ â†’ MongoDB ì¿¼ë¦¬ ìµœì í™”
â†’ 200ms â†’ 80ms ê°œì„  
```

### 3-3. êµ¬ì„± ìš”ì†Œ 4ê°€ì§€

#### â‘  Spring Boot X-Ray SDK (ì½”ë“œì— ì¶”ê°€)

**ì™œ í•„ìš”í•œê°€?**
- ì„œë¹„ìŠ¤ê°€ X-Rayì— íŠ¸ë ˆì´ìŠ¤ë¥¼ ë³´ë‚´ë ¤ë©´ SDK í•„ìš”
- íƒë°° ë³´ë‚´ë ¤ë©´ íƒë°° ìƒìê°€ í•„ìš”í•œ ê²ƒê³¼ ê°™ìŒ

**ì„¤ì • ë‚´ìš©:**
```xml
<!-- pom.xml -->
<dependency>
    <groupId>com.amazonaws</groupId>
    <artifactId>aws-xray-recorder-sdk-spring</artifactId>
    <version>2.15.0</version>
</dependency>
```

```java
// XRayConfig.java
@Configuration
public class XRayConfig {
    @Bean
    public Filter TracingFilter() {
        return new AWSXRayServletFilter("approval-request-service");
    }
}
```

**ë™ì‘ ë°©ì‹:**
```
1. ì‚¬ìš©ì ìš”ì²­ ë“¤ì–´ì˜´
2. AWSXRayServletFilterê°€ ìš”ì²­ ì‹œì‘ ì‹œê°„ ê¸°ë¡
3. ì„œë¹„ìŠ¤ ì²˜ë¦¬
4. ìš”ì²­ ì¢…ë£Œ ì‹œê°„ ê¸°ë¡
5. X-Ray Daemonìœ¼ë¡œ ì „ì†¡
```

**í™•ì¸ ë°©ë²•:**
```bash
# ì„œë¹„ìŠ¤ ë¡œê·¸ì—ì„œ X-Ray ì´ˆê¸°í™” í™•ì¸
kubectl logs -n erp-dev -l app=approval-request-service --tail=50 | grep -i xray

# ì¶œë ¥ ì˜ˆì‹œ:
# [INFO] AWS X-Ray Recorder initialized
#  X-Ray SDK ì •ìƒ ë™ì‘
```

#### â‘¡ X-Ray Daemon (íŠ¸ë ˆì´ìŠ¤ ìˆ˜ì§‘ê¸°)

**ì™œ í•„ìš”í•œê°€?**
- ì„œë¹„ìŠ¤ê°€ ë³´ë‚¸ íŠ¸ë ˆì´ìŠ¤ë¥¼ AWS X-Rayë¡œ ì „ë‹¬
- ìš°ì²´ë¶€ê°€ í¸ì§€ë¥¼ ìˆ˜ê±°í•´ì„œ ìš°ì²´êµ­ìœ¼ë¡œ ë³´ë‚´ëŠ” ê²ƒê³¼ ê°™ìŒ

**ë™ì‘ ë°©ì‹:**
```
Service â†’ X-Ray Daemon (UDP 2000) â†’ AWS X-Ray
```

**ì„¤ì • ë‚´ìš©:**
```yaml
# X-Ray DaemonSet
apiVersion: apps/v1
kind: DaemonSet  # ê° Nodeì— 1ê°œì”©
metadata:
  name: xray-daemon
  namespace: erp-dev
spec:
  template:
    spec:
      containers:
      - name: xray-daemon
        image: amazon/aws-xray-daemon:latest
        ports:
        - containerPort: 2000
          protocol: UDP
```

**í™•ì¸ ë°©ë²•:**
```bash
# X-Ray Daemon Pod í™•ì¸
kubectl get pods -n erp-dev -l app=xray-daemon

# ì¶œë ¥ ì˜ˆì‹œ:
# NAME                READY   STATUS    RESTARTS   AGE
# xray-daemon-xxxxx   1/1     Running   0          10m
# xray-daemon-yyyyy   1/1     Running   0          10m
# â†’ Node 2ê°œì´ë¯€ë¡œ Pod 2ê°œ 

# X-Ray Daemon ë¡œê·¸ í™•ì¸
kubectl logs -n erp-dev -l app=xray-daemon --tail=20

# ì¶œë ¥ ì˜ˆì‹œ:
# [Info] Initializing AWS X-Ray daemon 3.6.1
# [Info] Using region: ap-northeast-2
# [Info] Starting proxy http server on 0.0.0.0:2000
#  X-Ray Daemon ì •ìƒ ì‹¤í–‰
```

#### â‘¢ IAM ê¶Œí•œ (X-Ray Daemonì´ AWS X-Rayì— ì“¸ ìˆ˜ ìˆê²Œ)

**ì™œ í•„ìš”í•œê°€?**
- X-Ray Daemonì´ AWS X-Rayë¡œ íŠ¸ë ˆì´ìŠ¤ë¥¼ ë³´ë‚´ë ¤ë©´ ê¶Œí•œ í•„ìš”

**ì„¤ì • ë‚´ìš©:**
```json
{
  "Version": "2012-10-17",
  "Statement": [{
    "Effect": "Allow",
    "Action": [
      "xray:PutTraceSegments",      # íŠ¸ë ˆì´ìŠ¤ ë³´ë‚´ê¸°
      "xray:PutTelemetryRecords"    # í…”ë ˆë©”íŠ¸ë¦¬ ë³´ë‚´ê¸°
    ],
    "Resource": "*"
  }]
}
```

**í™•ì¸ ë°©ë²•:**
```bash
# IAM ê¶Œí•œ í™•ì¸
aws iam list-attached-role-policies --role-name erp-dev-eks-node-role --region ap-northeast-2

# ì¶œë ¥ì— "XRayDaemonPolicy" ìˆìœ¼ë©´ ì„±ê³µ 
```

#### â‘£ í™˜ê²½ë³€ìˆ˜ (ì„œë¹„ìŠ¤ê°€ X-Ray Daemon ì£¼ì†Œ ì•Œê²Œ)

**ì™œ í•„ìš”í•œê°€?**
- ì„œë¹„ìŠ¤ê°€ X-Ray Daemonì— íŠ¸ë ˆì´ìŠ¤ë¥¼ ë³´ë‚´ë ¤ë©´ ì£¼ì†Œ í•„ìš”
- í¸ì§€ ë³´ë‚´ë ¤ë©´ ìš°ì²´í†µ ìœ„ì¹˜ ì•Œì•„ì•¼ í•˜ëŠ” ê²ƒê³¼ ê°™ìŒ

**ì„¤ì • ë‚´ìš©:**
```yaml
# helm-chart/values-dev.yaml
services:
  approvalRequest:
    env:
      - name: AWS_XRAY_DAEMON_ADDRESS
        value: "xray-daemon.erp-dev.svc.cluster.local:2000"
```

**í™•ì¸ ë°©ë²•:**
```bash
# ì„œë¹„ìŠ¤ í™˜ê²½ë³€ìˆ˜ í™•ì¸
kubectl get deployment approval-request-service -n erp-dev \
  -o jsonpath='{.spec.template.spec.containers[0].env[?(@.name=="AWS_XRAY_DAEMON_ADDRESS")]}'

# ì¶œë ¥ ì˜ˆì‹œ:
# {"name":"AWS_XRAY_DAEMON_ADDRESS","value":"xray-daemon.erp-dev.svc.cluster.local:2000"}
#  í™˜ê²½ë³€ìˆ˜ ì„¤ì •ë¨
```

### 3-4. ì‹¤ì œ íŠ¸ë ˆì´ìŠ¤ í™•ì¸í•˜ê¸°

#### ë°©ë²• 1: AWS Console (ì›¹) - ì¶”ì²œ!

```
1. AWS Console ë¡œê·¸ì¸
2. X-Ray ì„œë¹„ìŠ¤ í´ë¦­
3. Service Map í´ë¦­

Service Map (ì„œë¹„ìŠ¤ ë§µ):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API Gateway â”‚ â†’   â”‚ approval-req â”‚ â†’   â”‚ employee â”‚
â”‚   50ms      â”‚     â”‚    200ms     â”‚     â”‚  150ms   â”‚
â”‚   100 req   â”‚     â”‚   100 req    â”‚     â”‚  100 req â”‚
â”‚   0% error  â”‚     â”‚   2% error   â”‚     â”‚  0% errorâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â†’ approval-request-serviceì—ì„œ 2% ì—ëŸ¬ ë°œìƒ 
â†’ í´ë¦­í•´ì„œ ìƒì„¸ í™•ì¸

4. Traces í´ë¦­

ê°œë³„ ìš”ì²­ ì¶”ì :
Request ID: abc123
Total Duration: 500ms
â”œâ”€ API Gateway: 50ms
â”œâ”€ approval-request-service: 200ms
â”‚  â”œâ”€ MongoDB query: 150ms  (ëŠë¦¼!)
â”‚  â””â”€ Kafka publish: 50ms
â”œâ”€ employee-service (Lambda): 150ms
â””â”€ notification-service: 100ms

â†’ MongoDB ì¿¼ë¦¬ê°€ ë³‘ëª© ì§€ì 
â†’ ì¸ë±ìŠ¤ ì¶”ê°€ë¡œ ìµœì í™” í•„ìš”
```

#### ë°©ë²• 2: AWS CLI (í„°ë¯¸ë„)

```bash
# Service Graph ì¡°íšŒ
aws xray get-service-graph \
  --start-time $(date -u -d '5 minutes ago' +%s) \
  --end-time $(date -u +%s) \
  --region ap-northeast-2

# Trace ì¡°íšŒ
aws xray get-trace-summaries \
  --start-time $(date -u -d '5 minutes ago' +%s) \
  --end-time $(date -u +%s) \
  --region ap-northeast-2
```

### 3-5. íŠ¸ë ˆì´ìŠ¤ ìƒì„±í•˜ê¸° (í…ŒìŠ¤íŠ¸)

**ì¤‘ìš”: X-RayëŠ” ìš”ì²­ì´ ìˆì–´ì•¼ íŠ¸ë ˆì´ìŠ¤ ìƒì„±ë¨**

```bash
# í…ŒìŠ¤íŠ¸ ìš”ì²­ ë³´ë‚´ê¸°
curl https://yvx3l9ifii.execute-api.ap-northeast-2.amazonaws.com/api/employees
curl https://yvx3l9ifii.execute-api.ap-northeast-2.amazonaws.com/api/approvals

# 1~2ë¶„ í›„ AWS Console â†’ X-Ray â†’ Service Map í™•ì¸
# íŠ¸ë ˆì´ìŠ¤ê°€ ë‚˜íƒ€ë‚¨ 
```

### 3-6. ì™„ë£Œ í™•ì¸ ì²´í¬ë¦¬ìŠ¤íŠ¸

```bash
#  1. X-Ray Daemon Pod í™•ì¸
kubectl get pods -n erp-dev -l app=xray-daemon
# 2ê°œ Pod Running í™•ì¸

#  2. X-Ray Service í™•ì¸
kubectl get svc xray-daemon -n erp-dev
# ClusterIP, UDP 2000 í™•ì¸

#  3. ì„œë¹„ìŠ¤ í™˜ê²½ë³€ìˆ˜ í™•ì¸
kubectl get deployment approval-request-service -n erp-dev \
  -o jsonpath='{.spec.template.spec.containers[0].env[?(@.name=="AWS_XRAY_DAEMON_ADDRESS")]}'
# xray-daemon.erp-dev.svc.cluster.local:2000 í™•ì¸

#  4. IAM ê¶Œí•œ í™•ì¸
aws iam list-attached-role-policies --role-name erp-dev-eks-node-role --region ap-northeast-2 | grep XRay
# XRayDaemonPolicy í™•ì¸

#  5. X-Ray Daemon ë¡œê·¸ í™•ì¸
kubectl logs -n erp-dev -l app=xray-daemon --tail=20
# "Starting proxy http server on 0.0.0.0:2000" í™•ì¸

#  6. í…ŒìŠ¤íŠ¸ ìš”ì²­ ë³´ë‚´ê¸°
curl https://yvx3l9ifii.execute-api.ap-northeast-2.amazonaws.com/api/employees

#  7. AWS Console â†’ X-Ray â†’ Service Map í™•ì¸
# 1~2ë¶„ í›„ íŠ¸ë ˆì´ìŠ¤ ë‚˜íƒ€ë‚¨
```

### 3-7. ì™œ ì´ê²Œ ì¤‘ìš”í•œê°€? (ì‹¤ë¬´ ê´€ì )

**ì‹œë‚˜ë¦¬ì˜¤ 1: ì„±ëŠ¥ ìµœì í™”**
```
Before:
- "ì„œë¹„ìŠ¤ê°€ ëŠë ¤ìš”"
- ì–´ë””ê°€ ëŠë¦°ì§€ ëª¨ë¦„
- ì „ì²´ ì½”ë“œ ë¦¬ë·° (ì‹œê°„ ë‚­ë¹„)

After:
- X-Ray Service Map í™•ì¸
- approval-request-serviceì˜ MongoDB ì¿¼ë¦¬ê°€ 200ms
- ì¸ë±ìŠ¤ ì¶”ê°€ â†’ 200ms â†’ 50ms ê°œì„  
```

**ì‹œë‚˜ë¦¬ì˜¤ 2: ì—ëŸ¬ ì¶”ì **
```
Before:
- "ê²°ì¬ ìš”ì²­ì´ ì•ˆ ë¼ìš”"
- ì–´ëŠ ì„œë¹„ìŠ¤ì—ì„œ ì—ëŸ¬ ë‚¬ëŠ”ì§€ ëª¨ë¦„
- ëª¨ë“  ì„œë¹„ìŠ¤ ë¡œê·¸ í™•ì¸ (ì‹œê°„ ë‚­ë¹„)

After:
- X-Ray Traces í™•ì¸
- employee-service (Lambda)ì—ì„œ 500 ì—ëŸ¬
- Lambda ë¡œê·¸ í™•ì¸ â†’ RDS ì—°ê²° ì‹¤íŒ¨
- RDS Security Group ìˆ˜ì • â†’ í•´ê²° 
```

**ì‹œë‚˜ë¦¬ì˜¤ 3: ì„œë¹„ìŠ¤ ì˜ì¡´ì„± íŒŒì•…**
```
X-Ray Service Map:
- approval-request â†’ employee (Lambda)
- approval-request â†’ notification
- approval-processing â†’ Kafka

â†’ employee-serviceë¥¼ ìˆ˜ì •í•˜ë©´ approval-requestì— ì˜í–¥
â†’ ë°°í¬ ì „ í…ŒìŠ¤íŠ¸ í•„ìˆ˜
```

---

## Step 4: ë‹¨ì¼ buildspec.yml ìƒì„± (60ë¶„)

### 4-1. ë³€ê²½ ê°ì§€ ë¡œì§ (Git diff)

**ì™œ í•„ìš”í•œê°€?**
- í˜„ì¬: ëª¨ë“  ì„œë¹„ìŠ¤ í•­ìƒ ë¹Œë“œ (ì‹œê°„ ë‚­ë¹„)
- ê°œì„ : ë³€ê²½ëœ ì„œë¹„ìŠ¤ë§Œ ë¹Œë“œ (ì‹œê°„ ë‹¨ì¶•)

### 4-2. ìµœì¢… buildspec.yml

```yaml
version: 0.2

env:
  # Parameter Store í†µí•©
  parameter-store:
    AWS_ACCOUNT_ID: /erp/dev/account-id
    AWS_REGION: /erp/dev/region
    EKS_CLUSTER_NAME: /erp/dev/eks/cluster-name
    ECR_REPOSITORY_PREFIX: /erp/dev/ecr/repository-prefix
    PROJECT_NAME: /erp/dev/project-name
    ENVIRONMENT: /erp/dev/environment

phases:
  install:
    commands:
      # Helm ì„¤ì¹˜
      - curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
      - helm version
      
      # yq ì„¤ì¹˜
      - wget https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 -O /usr/local/bin/yq
      - chmod +x /usr/local/bin/yq
      
      # kubectl í™•ì¸
      - kubectl version --client
  
  pre_build:
    commands:
      # ECR ë¡œê·¸ì¸
      - echo "Logging in to Amazon ECR..."
      - aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com
      
      # EKS kubeconfig
      - echo "Updating kubeconfig..."
      - aws eks update-kubeconfig --region $AWS_REGION --name $EKS_CLUSTER_NAME
      
      # ë³€ê²½ ê°ì§€ (Git diff)
      - echo "Detecting changed services..."
      - |
        if [ -z "$CODEBUILD_WEBHOOK_PREV_COMMIT" ]; then
          echo "First build, building all services"
          CHANGED_SERVICES="approval-request-service approval-processing-service notification-service"
          LAMBDA_CHANGED="false"
        else
          CHANGED_FILES=$(git diff --name-only $CODEBUILD_WEBHOOK_PREV_COMMIT $CODEBUILD_RESOLVED_SOURCE_VERSION)
          echo "Changed files: $CHANGED_FILES"
          
          CHANGED_SERVICES=""
          LAMBDA_CHANGED="false"
          
          if echo "$CHANGED_FILES" | grep -q "backend/employee-service/"; then
            LAMBDA_CHANGED="true"
          fi
          
          if echo "$CHANGED_FILES" | grep -q "backend/approval-request-service/"; then
            CHANGED_SERVICES="$CHANGED_SERVICES approval-request-service"
          fi
          if echo "$CHANGED_FILES" | grep -q "backend/approval-processing-service/"; then
            CHANGED_SERVICES="$CHANGED_SERVICES approval-processing-service"
          fi
          if echo "$CHANGED_FILES" | grep -q "backend/notification-service/"; then
            CHANGED_SERVICES="$CHANGED_SERVICES notification-service"
          fi
          
          if echo "$CHANGED_FILES" | grep -q "helm-chart/"; then
            echo "Helm Chart changed, deploying all EKS services"
            CHANGED_SERVICES="approval-request-service approval-processing-service notification-service"
          fi
        fi
      - echo "Services to build: $CHANGED_SERVICES"
      - echo "Lambda changed: $LAMBDA_CHANGED"
      - export CHANGED_SERVICES
      - export LAMBDA_CHANGED
      
      # ì´ë¯¸ì§€ íƒœê·¸
      - IMAGE_TAG=${CODEBUILD_RESOLVED_SOURCE_VERSION:0:7}
      - echo "Image tag: $IMAGE_TAG"
  
  build:
    commands:
      - echo "Build started on $(date)"
      
      # Lambda (Employee Service)
      - |
        if [ "$LAMBDA_CHANGED" = "true" ]; then
          echo "Building Lambda (Employee Service)..."
          cd backend/employee-service
          
          mvn clean package -DskipTests
          
          docker build -f Dockerfile.lambda -t employee-service-lambda:latest .
          docker tag employee-service-lambda:latest $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$ECR_REPOSITORY_PREFIX/employee-service-lambda:$IMAGE_TAG
          docker tag employee-service-lambda:latest $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$ECR_REPOSITORY_PREFIX/employee-service-lambda:latest
          
          docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$ECR_REPOSITORY_PREFIX/employee-service-lambda:$IMAGE_TAG
          docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$ECR_REPOSITORY_PREFIX/employee-service-lambda:latest
          
          aws lambda update-function-code \
            --function-name $PROJECT_NAME-$ENVIRONMENT-employee-service \
            --image-uri $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$ECR_REPOSITORY_PREFIX/employee-service-lambda:$IMAGE_TAG \
            --region $AWS_REGION
          
          cd ../..
        fi
      
      # EKS ì„œë¹„ìŠ¤
      - |
        for SERVICE in $CHANGED_SERVICES; do
          echo "Building $SERVICE..."
          cd backend/$SERVICE
          
          mvn clean package -DskipTests
          
          REPOSITORY_URI=$AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$ECR_REPOSITORY_PREFIX/$SERVICE
          docker build -t $REPOSITORY_URI:latest .
          docker tag $REPOSITORY_URI:latest $REPOSITORY_URI:$IMAGE_TAG
          
          docker push $REPOSITORY_URI:latest
          docker push $REPOSITORY_URI:$IMAGE_TAG
          
          # ECR ì´ë¯¸ì§€ ìŠ¤ìº”
          aws ecr start-image-scan \
            --repository-name $ECR_REPOSITORY_PREFIX/$SERVICE \
            --image-id imageTag=$IMAGE_TAG \
            --region $AWS_REGION || true
          
          cd ../..
        done
  
  post_build:
    commands:
      # ECR ìŠ¤ìº” ê²°ê³¼ í™•ì¸
      - echo "Checking ECR scan results..."
      - |
        for SERVICE in $CHANGED_SERVICES; do
          echo "Waiting for scan results for $SERVICE..."
          SCAN_STATUS="IN_PROGRESS"
          RETRY_COUNT=0
          MAX_RETRIES=30
          
          while [ "$SCAN_STATUS" = "IN_PROGRESS" ] && [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            sleep 10
            SCAN_STATUS=$(aws ecr describe-image-scan-findings \
              --repository-name $ECR_REPOSITORY_PREFIX/$SERVICE \
              --image-id imageTag=$IMAGE_TAG \
              --region $AWS_REGION \
              --query 'imageScanStatus.status' \
              --output text 2>/dev/null || echo "IN_PROGRESS")
            RETRY_COUNT=$((RETRY_COUNT + 1))
          done
          
          if [ "$SCAN_STATUS" = "COMPLETE" ]; then
            CRITICAL=$(aws ecr describe-image-scan-findings \
              --repository-name $ECR_REPOSITORY_PREFIX/$SERVICE \
              --image-id imageTag=$IMAGE_TAG \
              --region $AWS_REGION \
              --query 'imageScanFindings.findingSeverityCounts.CRITICAL' \
              --output text 2>/dev/null || echo "0")
            
            if [ "$CRITICAL" != "None" ] && [ "$CRITICAL" != "0" ]; then
              echo " Critical vulnerabilities found in $SERVICE: $CRITICAL"
              exit 1
            else
              echo " No critical vulnerabilities in $SERVICE"
            fi
          fi
        done
      
      # Helm values ì—…ë°ì´íŠ¸
      - echo "Updating Helm values..."
      - |
        for SERVICE in $CHANGED_SERVICES; do
          SERVICE_KEY=$(echo $SERVICE | sed 's/-service$//' | sed 's/-\([a-z]\)/\U\1/g' | sed 's/^./\L&/')
          yq eval ".services.$SERVICE_KEY.image.tag = \"$IMAGE_TAG\"" -i helm-chart/values-dev.yaml
          echo "Updated $SERVICE_KEY to $IMAGE_TAG"
        done
      
      # Helm ë°°í¬
      - echo "Deploying to EKS with Helm..."
      - |
        helm upgrade --install erp-microservices helm-chart/ \
          -f helm-chart/values-dev.yaml \
          -n erp-dev \
          --create-namespace \
          --wait \
          --timeout 5m
      
      # ë°°í¬ í™•ì¸
      - kubectl get pods -n erp-dev
      - kubectl get svc -n erp-dev
      - helm history erp-microservices -n erp-dev
      
      - echo "Build completed on $(date)"

artifacts:
  files:
    - helm-chart/**/*
  name: helm-chart-$IMAGE_TAG

cache:
  paths:
    - '/root/.m2/**/*'
```

### 4-3. ê¸°ì¡´ buildspec.yml ì‚­ì œ

```bash
cd /mnt/c/Users/Lethe/Desktop/ì·¨ì—…ì¤€ë¹„/erp-project

# ë°±ì—…
mkdir -p backup-buildspec
cp backend/approval-request-service/buildspec.yml backup-buildspec/ 2>/dev/null || true
cp backend/approval-processing-service/buildspec.yml backup-buildspec/ 2>/dev/null || true
cp backend/notification-service/buildspec.yml backup-buildspec/ 2>/dev/null || true

# ì‚­ì œ
rm -f backend/approval-request-service/buildspec.yml
rm -f backend/approval-processing-service/buildspec.yml
rm -f backend/notification-service/buildspec.yml
```

### 4-4. ë£¨íŠ¸ì— buildspec.yml ìƒì„±

```bash
# ìœ„ì˜ buildspec.yml ë‚´ìš©ì„ ë£¨íŠ¸ì— ìƒì„±
cat > buildspec.yml << 'EOF'
# (ìœ„ì˜ ì „ì²´ ë‚´ìš©)
EOF
```

### 4-5. Git ì»¤ë°‹

```bash
git add buildspec.yml
git add helm-chart/
git rm backend/*/buildspec.yml 2>/dev/null || true
git commit -m "feat: Unified buildspec with 7 CodePipeline features"
git push origin main
```

---

##  ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Step 1: Parameter Store
- [ ] Terraformìœ¼ë¡œ 6ê°œ Parameter ìƒì„±
- [ ] terraform output í™•ì¸
- [ ] CodeBuild Role SSM ê¶Œí•œ í™•ì¸

### Step 2: CloudWatch Logs
- [ ] CodeBuild CloudWatch Logs í™•ì¸ (07ë‹¨ê³„ì—ì„œ ì„¤ì •)
- [ ] Fluent Bit DaemonSet ë°°í¬
- [ ] CloudWatch Logs ê·¸ë£¹ í™•ì¸


### Step 3: X-Ray íŠ¸ë ˆì´ì‹±
- [ ] Spring Boot X-Ray SDK ì¶”ê°€ (pom.xml)
- [ ] XRayConfig.java ìƒì„±
- [ ] X-Ray DaemonSet ë°°í¬
- [ ] Helm Chart X-Ray í™˜ê²½ ë³€ìˆ˜ ì¶”ê°€
- [ ] X-Ray ì½˜ì†”ì—ì„œ Service Map í™•ì¸

### Step 4: ë‹¨ì¼ buildspec.yml
- [ ] ë£¨íŠ¸ì— buildspec.yml ìƒì„±
- [ ] ë³€ê²½ ê°ì§€ ë¡œì§ ë™ì‘ í™•ì¸
- [ ] ê¸°ì¡´ buildspec.yml 3ê°œ ì‚­ì œ
- [ ] Git ì»¤ë°‹ ì™„ë£Œ

---

## Step 5: CloudWatch Alarm ì¶”ê°€ (ì‹¤ë¬´ í•„ìˆ˜) â­â­â­â­â­

### 5-1. ì™œ í•„ìš”í•œê°€?

**í˜„ì¬ ìƒí™©:**
- CloudWatch Logsë¡œ ë¡œê·¸ ìˆ˜ì§‘ âœ…
- X-Rayë¡œ íŠ¸ë ˆì´ì‹± âœ…
- **í•˜ì§€ë§Œ ì•Œë¦¼ì´ ì—†ìŒ** âŒ

**ë¬¸ì œ:**
```
ERROR ë¡œê·¸ ë°œìƒ
  â†“
ì•„ë¬´ë„ ëª¨ë¦„
  â†“
ì¥ì•  ì§€ì†
```

**CloudWatch Alarm ì¶”ê°€ ì‹œ:**
```
ERROR ë¡œê·¸ ë°œìƒ
  â†“
CloudWatch Alarm ê°ì§€
  â†“
SNS â†’ Email ì•Œë¦¼
  â†“
ì¦‰ì‹œ ëŒ€ì‘
```

---

### 5-2. Terraformìœ¼ë¡œ CloudWatch Alarm ìƒì„±

#### 5-2-1. SNS Topic ìƒì„±

```bash
cd infrastructure/terraform/dev/erp-dev-CloudWatch
```

**sns.tf ìƒì„±:**
```hcl
# SNS Topic for Alarms
resource "aws_sns_topic" "erp_alarms" {
  name = "${var.project_name}-${var.environment}-alarms"
  
  tags = {
    Name        = "${var.project_name}-${var.environment}-alarms"
    Environment = var.environment
    Project     = var.project_name
  }
}

# SNS Subscription (Email)
resource "aws_sns_topic_subscription" "erp_alarms_email" {
  topic_arn = aws_sns_topic.erp_alarms.arn
  protocol  = "email"
  endpoint  = var.alarm_email  # ì´ë©”ì¼ ì£¼ì†Œ
}

output "sns_topic_arn" {
  value = aws_sns_topic.erp_alarms.arn
}
```

**variables.tf ìˆ˜ì •:**
```hcl
variable "alarm_email" {
  description = "Email address for alarm notifications"
  type        = string
  default     = "your-email@example.com"  # ë³¸ì¸ ì´ë©”ì¼ë¡œ ë³€ê²½
}
```

---

#### 5-2-2. CloudWatch Metric Filter ìƒì„±

**log-metric-filters.tf ìƒì„±:**
```hcl
# ERROR ë¡œê·¸ ì¹´ìš´íŠ¸ ë©”íŠ¸ë¦­
resource "aws_cloudwatch_log_metric_filter" "error_logs" {
  name           = "${var.project_name}-${var.environment}-error-count"
  log_group_name = data.terraform_remote_state.eks.outputs.cloudwatch_log_group_name
  
  # ERROR ë ˆë²¨ ë¡œê·¸ íŒ¨í„´
  pattern = "[time, request_id, level = ERROR*, ...]"
  
  metric_transformation {
    name      = "ErrorCount"
    namespace = "ERP/Application"
    value     = "1"
    default_value = 0
  }
}

# Pod ì¬ì‹œì‘ ë©”íŠ¸ë¦­
resource "aws_cloudwatch_log_metric_filter" "pod_restarts" {
  name           = "${var.project_name}-${var.environment}-pod-restarts"
  log_group_name = data.terraform_remote_state.eks.outputs.cloudwatch_log_group_name
  
  # Pod ì¬ì‹œì‘ íŒ¨í„´
  pattern = "[time, stream, log_type = *restart* || log_type = *killed* || log_type = *crash*]"
  
  metric_transformation {
    name      = "PodRestartCount"
    namespace = "ERP/Application"
    value     = "1"
    default_value = 0
  }
}
```

---

#### 5-2-3. CloudWatch Alarm ìƒì„±

**alarms.tf ìƒì„±:**
```hcl
# ERROR ë¡œê·¸ ì•ŒëŒ (5ë¶„ ë™ì•ˆ 10íšŒ ì´ìƒ)
resource "aws_cloudwatch_metric_alarm" "high_error_rate" {
  alarm_name          = "${var.project_name}-${var.environment}-high-error-rate"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = "1"
  metric_name         = "ErrorCount"
  namespace           = "ERP/Application"
  period              = "300"  # 5ë¶„
  statistic           = "Sum"
  threshold           = "10"
  alarm_description   = "ERROR ë¡œê·¸ê°€ 5ë¶„ ë™ì•ˆ 10íšŒ ì´ìƒ ë°œìƒ"
  treat_missing_data  = "notBreaching"
  
  alarm_actions = [aws_sns_topic.erp_alarms.arn]
  ok_actions    = [aws_sns_topic.erp_alarms.arn]
  
  tags = {
    Name        = "${var.project_name}-${var.environment}-high-error-rate"
    Environment = var.environment
    Severity    = "High"
  }
}

# Pod ì¬ì‹œì‘ ì•ŒëŒ (10ë¶„ ë™ì•ˆ 3íšŒ ì´ìƒ)
resource "aws_cloudwatch_metric_alarm" "pod_restart_alarm" {
  alarm_name          = "${var.project_name}-${var.environment}-pod-restarts"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = "1"
  metric_name         = "PodRestartCount"
  namespace           = "ERP/Application"
  period              = "600"  # 10ë¶„
  statistic           = "Sum"
  threshold           = "3"
  alarm_description   = "Podê°€ 10ë¶„ ë™ì•ˆ 3íšŒ ì´ìƒ ì¬ì‹œì‘"
  treat_missing_data  = "notBreaching"
  
  alarm_actions = [aws_sns_topic.erp_alarms.arn]
  ok_actions    = [aws_sns_topic.erp_alarms.arn]
  
  tags = {
    Name        = "${var.project_name}-${var.environment}-pod-restarts"
    Environment = var.environment
    Severity    = "Critical"
  }
}

# Lambda ì—ëŸ¬ìœ¨ ì•ŒëŒ (5ë¶„ ë™ì•ˆ ì—ëŸ¬ìœ¨ 5% ì´ìƒ)
resource "aws_cloudwatch_metric_alarm" "lambda_error_rate" {
  alarm_name          = "${var.project_name}-${var.environment}-lambda-error-rate"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = "1"
  threshold           = "5"
  alarm_description   = "Lambda ì—ëŸ¬ìœ¨ì´ 5% ì´ìƒ"
  treat_missing_data  = "notBreaching"
  
  metric_query {
    id          = "error_rate"
    expression  = "(errors / invocations) * 100"
    label       = "Error Rate"
    return_data = true
  }
  
  metric_query {
    id = "errors"
    metric {
      metric_name = "Errors"
      namespace   = "AWS/Lambda"
      period      = "300"
      stat        = "Sum"
      dimensions = {
        FunctionName = "${var.project_name}-${var.environment}-employee-service"
      }
    }
  }
  
  metric_query {
    id = "invocations"
    metric {
      metric_name = "Invocations"
      namespace   = "AWS/Lambda"
      period      = "300"
      stat        = "Sum"
      dimensions = {
        FunctionName = "${var.project_name}-${var.environment}-employee-service"
      }
    }
  }
  
  alarm_actions = [aws_sns_topic.erp_alarms.arn]
  ok_actions    = [aws_sns_topic.erp_alarms.arn]
  
  tags = {
    Name        = "${var.project_name}-${var.environment}-lambda-error-rate"
    Environment = var.environment
    Severity    = "High"
  }
}
```

---

#### 5-2-4. Remote State ì„¤ì •

**data.tf ìƒì„±:**
```hcl
data "terraform_remote_state" "eks" {
  backend = "s3"
  config = {
    bucket = "erp-terraform-state-806332783810"
    key    = "dev/erp-dev-EKS/terraform.tfstate"
    region = "ap-northeast-2"
  }
}
```

---

### 5-3. Terraform ì‹¤í–‰

```bash
cd infrastructure/terraform/dev/erp-dev-CloudWatch

# ì´ˆê¸°í™”
terraform init

# ê³„íš í™•ì¸
terraform plan

# ì ìš©
terraform apply -auto-approve
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
Apply complete! Resources: 5 added, 0 changed, 0 destroyed.

Outputs:

sns_topic_arn = "arn:aws:sns:ap-northeast-2:806332783810:erp-dev-alarms"
```

---

### 5-4. SNS êµ¬ë… í™•ì¸

**ì´ë©”ì¼ í™•ì¸:**
1. AWSì—ì„œ êµ¬ë… í™•ì¸ ì´ë©”ì¼ ë°œì†¡
2. ì´ë©”ì¼ ì—´ê¸°
3. "Confirm subscription" í´ë¦­

**í™•ì¸:**
```bash
aws sns list-subscriptions-by-topic \
  --topic-arn arn:aws:sns:ap-northeast-2:806332783810:erp-dev-alarms \
  --region ap-northeast-2
```

---

### 5-5. ì•ŒëŒ í…ŒìŠ¤íŠ¸

#### í…ŒìŠ¤íŠ¸ 1: ERROR ë¡œê·¸ ìƒì„±

```bash
# Podì— ì ‘ì†
kubectl exec -it deployment/approval-request-service -n erp-dev -- /bin/sh

# ì—ëŸ¬ ë¡œê·¸ ìƒì„± (Java ì• í”Œë¦¬ì¼€ì´ì…˜ ë‚´ë¶€ì—ì„œ)
# ë˜ëŠ” ê°„ë‹¨íˆ í…ŒìŠ¤íŠ¸ìš© ì—ëŸ¬ ë¡œê·¸ ì¶œë ¥
for i in {1..15}; do
  echo "$(date) ERROR Test error message $i"
  sleep 1
done
```

**5ë¶„ í›„ ì´ë©”ì¼ í™•ì¸:**
```
Subject: ALARM: "erp-dev-high-error-rate" in Asia Pacific (Seoul)

You are receiving this email because your Amazon CloudWatch Alarm 
"erp-dev-high-error-rate" in the Asia Pacific (Seoul) region has 
entered the ALARM state.

Alarm Details:
- State Change: OK -> ALARM
- Reason: Threshold Crossed: 1 datapoint [15.0] was greater than 
  the threshold (10.0).
```

---

#### í…ŒìŠ¤íŠ¸ 2: Pod ì¬ì‹œì‘

```bash
# Pod ê°•ì œ ì‚­ì œ (ì¬ì‹œì‘ ìœ ë°œ)
kubectl delete pod -l app=approval-request-service -n erp-dev

# 3ë²ˆ ë°˜ë³µ
kubectl delete pod -l app=approval-processing-service -n erp-dev
kubectl delete pod -l app=notification-service -n erp-dev
```

**10ë¶„ í›„ ì´ë©”ì¼ í™•ì¸:**
```
Subject: ALARM: "erp-dev-pod-restarts" in Asia Pacific (Seoul)

Alarm Details:
- State Change: OK -> ALARM
- Reason: Threshold Crossed: 1 datapoint [3.0] was greater than 
  the threshold (3.0).
```

---

### 5-6. CloudWatch Console í™•ì¸

**Alarms í™•ì¸:**
```bash
# ë¸Œë¼ìš°ì €ì—ì„œ
https://ap-northeast-2.console.aws.amazon.com/cloudwatch/home?region=ap-northeast-2#alarmsV2:
```

**í™•ì¸ í•­ëª©:**
- erp-dev-high-error-rate: OK ìƒíƒœ
- erp-dev-pod-restarts: OK ìƒíƒœ
- erp-dev-lambda-error-rate: OK ìƒíƒœ

---

### 5-7. ë©´ì ‘ ì–´í•„ í¬ì¸íŠ¸

**Q: ëª¨ë‹ˆí„°ë§ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?**

**A:**
"3ë‹¨ê³„ë¡œ ëª¨ë‹ˆí„°ë§í•©ë‹ˆë‹¤. ì²«ì§¸, CloudWatch Logsë¡œ ëª¨ë“  Pod ë¡œê·¸ë¥¼ ì¤‘ì•™ ì§‘ì¤‘í•©ë‹ˆë‹¤. ë‘˜ì§¸, CloudWatch Alarmìœ¼ë¡œ ERROR ë¡œê·¸ê°€ 5ë¶„ ë™ì•ˆ 10íšŒ ì´ìƒ ë°œìƒí•˜ê±°ë‚˜ Podê°€ 10ë¶„ ë™ì•ˆ 3íšŒ ì´ìƒ ì¬ì‹œì‘í•˜ë©´ SNSë¡œ ì´ë©”ì¼ ì•Œë¦¼ì„ ë°›ìŠµë‹ˆë‹¤. ì…‹ì§¸, X-Rayë¡œ ì„œë¹„ìŠ¤ ê°„ íŠ¸ë ˆì´ì‹±ì„ ì¶”ì í•˜ì—¬ ë³‘ëª© ì§€ì ì„ íŒŒì•…í•©ë‹ˆë‹¤."

**Q: ì¥ì•  ë°œìƒ ì‹œ ì–´ë–»ê²Œ ëŒ€ì‘í•˜ë‚˜ìš”?**

**A:**
"CloudWatch Alarmì´ ì´ë©”ì¼ë¡œ ì•Œë¦¼ì„ ë³´ë‚´ë©´, CloudWatch Logs Insightsë¡œ ì—ëŸ¬ ë¡œê·¸ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì›ì¸ì„ íŒŒì•…í•©ë‹ˆë‹¤. X-Ray Service Mapì—ì„œ ì–´ëŠ ì„œë¹„ìŠ¤ì—ì„œ ì§€ì—°ì´ ë°œìƒí–ˆëŠ”ì§€ í™•ì¸í•˜ê³ , kubectl logsë¡œ ìƒì„¸ ë¡œê·¸ë¥¼ í™•ì¸í•©ë‹ˆë‹¤. ë¬¸ì œ í•´ê²° í›„ Alarmì´ ìë™ìœ¼ë¡œ OK ìƒíƒœë¡œ ëŒì•„ê°‘ë‹ˆë‹¤."

---

##  ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

### CGV vs ERP ë¹„êµ

| ê¸°ëŠ¥ | CGV (GitLab CI) | ERP (CodePipeline) | ì°¨ë³„í™” í¬ì¸íŠ¸ |
|------|----------------|-------------------|--------------|
| Secret ê´€ë¦¬ | GitLab Variables |  AWS Secrets Manager | RDS ìë™ ë¡œí…Œì´ì…˜ |
| ì„¤ì • ê´€ë¦¬ | .gitlab-ci.yml í•˜ë“œì½”ë”© |  Parameter Store | ì¤‘ì•™ ê´€ë¦¬, í™˜ê²½ë³„ ë¶„ë¦¬ |
| ë¡œê·¸ ê´€ë¦¬ | GitLab Logs |  CloudWatch Logs | ì˜êµ¬ ë³´ê´€, í†µí•© ê²€ìƒ‰ |
| **ì•Œë¦¼ ê´€ë¦¬** | **ìˆ˜ë™** | ** CloudWatch Alarm + SNS** | **ì‹¤ì‹œê°„ ì´ë©”ì¼ ì•Œë¦¼** |
| ì´ë¯¸ì§€ ìŠ¤ìº” | ìˆ˜ë™ |  ECR ìë™ ìŠ¤ìº” | Critical ë°œê²¬ ì‹œ ë°°í¬ ì¤‘ë‹¨ |
| íŠ¸ë ˆì´ì‹± | ì—†ìŒ |  X-Ray | Service Map, ë³‘ëª© ë¶„ì„ |
| ë³€ê²½ ê°ì§€ | ì „ì²´ ë¹Œë“œ |  Git diff | ë³€ê²½ëœ ì„œë¹„ìŠ¤ë§Œ ë¹Œë“œ |
| ë°°í¬ ë°©ì‹ | kubectl set image |  helm upgrade | Manifests ìë™ ë°˜ì˜ |

**ê²°ë¡ : CodePipelineì˜ AWS ë„¤ì´í‹°ë¸Œ í†µí•©ì„ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ CGV ìˆ˜ì¤€ ì´ˆê³¼ ë‹¬ì„±!**

---

##  ë‹¤ìŒ ë‹¨ê³„

**06ë‹¨ê³„ ì™„ë£Œ!**

**ë‹¤ìŒ íŒŒì¼ì„ ì½ìœ¼ì„¸ìš”:**
â†’ **07_CODEPIPELINE.md**

```bash
cd /mnt/c/Users/Lethe/Desktop/ì·¨ì—…ì¤€ë¹„/erp-project/re_build
cat 07_CODEPIPELINE.md
```

---

**"7ê°€ì§€ CodePipeline ê°•ì ì„ ëª¨ë‘ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤. ì´ì œ CGVì™€ ëŒ€ë“±í•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤!"**


---

## ğŸ” X-Ray íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ì „ì²´ ê³¼ì • (ì‹¤ì œ êµ¬í˜„ ê¸°ë¡)

### âŒ **ì´ˆê¸° ë¬¸ì œ: X-Ray íŠ¸ë ˆì´ìŠ¤ê°€ ì „ì†¡ë˜ì§€ ì•ŠìŒ**

**ì¦ìƒ:**
```bash
# X-Ray Daemon ë¡œê·¸ í™•ì¸
kubectl logs -n erp-dev xray-daemon-xxxxx

# ì¶œë ¥:
# [Info] Starting proxy http server on 0.0.0.0:2000
# â†’ ì´í›„ ì•„ë¬´ ë¡œê·¸ ì—†ìŒ (íŠ¸ë ˆì´ìŠ¤ ë¯¸ìˆ˜ì‹ )
```

**ì›ì¸ ë¶„ì„:**
1. âœ… X-Ray DaemonSet ì •ìƒ ì‹¤í–‰ (2ê°œ Pod Running)
2. âœ… í™˜ê²½ë³€ìˆ˜ ì„¤ì •ë¨ (`AWS_XRAY_DAEMON_ADDRESS`)
3. âœ… pom.xmlì— X-Ray SDK ì¡´ì¬
4. âœ… XRayConfig.java ì¡´ì¬
5. âŒ **Spring Boot ë¡œê·¸ì— X-Ray ì´ˆê¸°í™” ë©”ì‹œì§€ ì—†ìŒ**

### ğŸ”§ **í•´ê²° ê³¼ì •**

#### Step 1: XRayConfigì— ë¡œê¹… ì¶”ê°€

**ë¬¸ì œ:** X-Rayê°€ ì´ˆê¸°í™”ë˜ëŠ”ì§€ í™•ì¸ ë¶ˆê°€

**í•´ê²°:**
```java
// XRayConfig.java
@Configuration
public class XRayConfig {
    
    private static final Logger logger = LoggerFactory.getLogger(XRayConfig.class);
    
    @PostConstruct
    public void init() {
        logger.info("=== X-Ray Configuration Initializing ===");
        logger.info("X-Ray Daemon Address: {}", System.getenv("AWS_XRAY_DAEMON_ADDRESS"));
        
        AWSXRayRecorderBuilder builder = AWSXRayRecorderBuilder.standard();
        AWSXRay.setGlobalRecorder(builder.build());
        
        logger.info("=== X-Ray Recorder Initialized Successfully ===");
    }
    
    @Bean
    public Filter TracingFilter() {
        logger.info("=== X-Ray Servlet Filter Created ===");
        return new AWSXRayServletFilter("approval-request-service");
    }
}
```

**ê²°ê³¼:**
```bash
kubectl logs -n erp-dev approval-request-service-xxx --tail=100 | grep "=== X-Ray"

# ì¶œë ¥:
# === X-Ray Configuration Initializing ===
# === X-Ray Recorder Initialized Successfully ===
# === X-Ray Servlet Filter Created ===
# âœ… X-Ray ì´ˆê¸°í™” í™•ì¸!
```

#### Step 2: ì‹¤ì œ ìš”ì²­ìœ¼ë¡œ íŠ¸ë ˆì´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸

**ë¬¸ì œ:** employee-service (Lambda)ë¡œ ìš”ì²­ ì‹œ íŠ¸ë ˆì´ìŠ¤ ë¯¸ìƒì„±

**ì›ì¸:** LambdaëŠ” ë³„ë„ X-Ray ì„¤ì • í•„ìš”, EKS ì„œë¹„ìŠ¤ë§Œ í…ŒìŠ¤íŠ¸í•´ì•¼ í•¨

**í•´ê²°:**
```bash
# approval-request-serviceì— ì§ì ‘ ìš”ì²­
curl https://yvx3l9ifii.execute-api.ap-northeast-2.amazonaws.com/api/approvals

# X-Ray Daemon ë¡œê·¸ í™•ì¸
kubectl logs -n erp-dev xray-daemon-xxxxx --since=30s

# ì¶œë ¥:
# [Info] Successfully sent batch of 1 segments (0.453 seconds)
# âœ… íŠ¸ë ˆì´ìŠ¤ ì „ì†¡ ì„±ê³µ!
```

#### Step 3: ëª¨ë“  ì„œë¹„ìŠ¤ì— ì ìš©

**ì‘ì—… ë‚´ìš©:**
1. approval-processing-service/XRayConfig.java ì—…ë°ì´íŠ¸
2. notification-service/XRayConfig.java ì—…ë°ì´íŠ¸
3. ì´ë¯¸ì§€ ì¬ë¹Œë“œ & ECR í‘¸ì‹œ
4. Pod ì¬ì‹œì‘

**ê²°ê³¼:**
```bash
# 30ê°œ ìš”ì²­ ì „ì†¡
for i in {1..30}; do 
  curl -s https://yvx3l9ifii.execute-api.ap-northeast-2.amazonaws.com/api/approvals > /dev/null
  sleep 2
done

# X-Ray Daemon ë¡œê·¸
kubectl logs -n erp-dev xray-daemon-xxxxx --since=2m

# ì¶œë ¥:
# [Info] Successfully sent batch of 1 segments (0.042 seconds)
# [Info] Successfully sent batch of 1 segments (0.016 seconds)
# [Info] Successfully sent batch of 1 segments (0.020 seconds)
# ... (ê³„ì† ì „ì†¡ ì¤‘)
# âœ… ëª¨ë“  ì„œë¹„ìŠ¤ íŠ¸ë ˆì´ìŠ¤ ì „ì†¡ ì„±ê³µ!
```

---

### ğŸ“Š **X-Ray vs CloudWatch Logs ì°¨ì´ì **

| í•­ëª© | CloudWatch Logs | X-Ray |
|------|----------------|-------|
| **ëª©ì ** | ë¡œê·¸ ì €ì¥ ë° ê²€ìƒ‰ | ë¶„ì‚° íŠ¸ë ˆì´ì‹± |
| **ìˆ˜ì§‘ ëŒ€ìƒ** | í…ìŠ¤íŠ¸ ë¡œê·¸ (stdout/stderr) | HTTP ìš”ì²­ íë¦„ |
| **ì‚¬ìš© ì‹œì ** | ì—ëŸ¬ ë¡œê·¸ ë¶„ì„, ë””ë²„ê¹… | ì„±ëŠ¥ ë³‘ëª© ë¶„ì„, ì„œë¹„ìŠ¤ ì˜ì¡´ì„± íŒŒì•… |
| **ì‹œê°í™”** | í…ìŠ¤íŠ¸ ê²€ìƒ‰, ê·¸ë˜í”„ | Service Map (ë…¸ë“œ + ì—£ì§€) |
| **ì˜ˆì‹œ** | "ERROR: Connection failed" | "approval-request â†’ employee (200ms)" |
| **ì–¸ì œ ì‚¬ìš©?** | ë¬´ì—‡ì´ ì˜ëª»ë˜ì—ˆëŠ”ì§€ (What) | ì–´ë””ê°€ ëŠë¦°ì§€ (Where) |

**ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ:**

**CloudWatch Logs:**
```bash
# ì—ëŸ¬ ë¡œê·¸ ê²€ìƒ‰
aws logs tail /aws/eks/erp-dev/application --since 1h | grep ERROR

# ì¶œë ¥:
# 2025-12-29 04:00:00 ERROR Connection to MongoDB failed
# 2025-12-29 04:05:00 ERROR Kafka producer timeout
# â†’ ë¬´ì—‡ì´ ì˜ëª»ë˜ì—ˆëŠ”ì§€ íŒŒì•…
```

**X-Ray:**
```
AWS Console â†’ X-Ray â†’ Service Map

í´ë¼ì´ì–¸íŠ¸ â†’ approval-request (1.2ì´ˆ) â†’ MongoDB (0.8ì´ˆ)
                    â†“
              notification (0.3ì´ˆ) â†’ Redis (0.1ì´ˆ)

â†’ MongoDB ì¿¼ë¦¬ê°€ ëŠë¦¼ (0.8ì´ˆ)
â†’ ì¸ë±ìŠ¤ ì¶”ê°€ í•„ìš”
â†’ ì–´ë””ê°€ ëŠë¦°ì§€ íŒŒì•…
```

---

### âœ… **X-Ray ë™ì‘ ì¡°ê±´ (ì¤‘ìš”!)**

X-Rayê°€ íŠ¸ë ˆì´ìŠ¤ë¥¼ ìƒì„±í•˜ë ¤ë©´:

1. âœ… **Spring Bootì— X-Ray SDK ì¶”ê°€** (pom.xml)
2. âœ… **XRayConfig.java ìƒì„±** (Filter ë“±ë¡)
3. âœ… **X-Ray DaemonSet ë°°í¬** (Helm Chart)
4. âœ… **í™˜ê²½ë³€ìˆ˜ ì„¤ì •** (AWS_XRAY_DAEMON_ADDRESS)
5. âœ… **IAM ê¶Œí•œ** (EKS Node Roleì— XRay ê¶Œí•œ)
6. âœ… **ì‹¤ì œ HTTP ìš”ì²­** (íŠ¸ë ˆì´ìŠ¤ëŠ” ìš”ì²­ì´ ìˆì–´ì•¼ ìƒì„±ë¨)

**ì£¼ì˜:** X-RayëŠ” **HTTP ìš”ì²­ì´ ë“¤ì–´ì™€ì•¼** íŠ¸ë ˆì´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤!
- Podë§Œ ì‹¤í–‰ ì¤‘ â†’ íŠ¸ë ˆì´ìŠ¤ ì—†ìŒ
- API ìš”ì²­ â†’ íŠ¸ë ˆì´ìŠ¤ ìƒì„± â†’ X-Ray Daemon â†’ AWS X-Ray

---

### ğŸ¯ **Helm Chart vs CLI ì‘ì—… í™•ì¸**

**ì§ˆë¬¸:** "í—¬ë¦„ì°¨íŠ¸ë‚˜ í…Œë¼í¼ ì½”ë“œì— ì¶”ê°€í•˜ì§€ì•Šê³  CLIë¡œ ì‘ì—…í•œê±° ë­ ì—†ì§€?"

**ë‹µë³€:** âœ… **ëª¨ë“  ì‘ì—…ì´ Helm Chart ë˜ëŠ” Terraformì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤!**

#### Helm Chartì— í¬í•¨ëœ ê²ƒ:

1. **xray-daemonset.yaml** (templates/)
   ```yaml
   apiVersion: apps/v1
   kind: DaemonSet
   metadata:
     name: xray-daemon
   ```
   - âœ… Helm Chartì— í¬í•¨
   - âœ… `helm upgrade` ëª…ë ¹ìœ¼ë¡œ ë°°í¬

2. **values-dev.yaml** (X-Ray ì„¤ì •)
   ```yaml
   xray:
     enabled: true
     image:
       repository: amazon/aws-xray-daemon
       tag: latest
   ```
   - âœ… Helm Chartì— í¬í•¨

3. **ì„œë¹„ìŠ¤ í™˜ê²½ë³€ìˆ˜** (values-dev.yaml)
   ```yaml
   services:
     approvalRequest:
       env:
         - name: AWS_XRAY_DAEMON_ADDRESS
           value: "xray-daemon.erp-dev.svc.cluster.local:2000"
   ```
   - âœ… Helm Chartì— í¬í•¨

#### Terraformì— í¬í•¨ëœ ê²ƒ:

1. **IAM ê¶Œí•œ** (erp-dev-IAM/eks-node-role/)
   ```hcl
   resource "aws_iam_role_policy_attachment" "eks_node_xray" {
     role       = aws_iam_role.eks_node.name
     policy_arn = "arn:aws:iam::aws:policy/AWSXRayDaemonWriteAccess"
   }
   ```
   - âœ… Terraformì— í¬í•¨

#### CLIë¡œë§Œ í•œ ì‘ì—… (ì„ì‹œ):

1. **Pod ì‚­ì œ ë° ì¬ì‹œì‘**
   ```bash
   kubectl delete pods -n erp-dev -l app=approval-request-service
   ```
   - âš ï¸ ì„ì‹œ ì‘ì—… (ì´ë¯¸ì§€ ì—…ë°ì´íŠ¸ í›„ ì¬ì‹œì‘ìš©)
   - âœ… Helm Chartì˜ replicaCountê°€ ìë™ìœ¼ë¡œ ì¬ìƒì„±

2. **ì´ë¯¸ì§€ íƒœê·¸ ë³€ê²½ (í…ŒìŠ¤íŠ¸ìš©)**
   ```bash
   kubectl set image deployment/approval-request-service ...
   ```
   - âš ï¸ ì„ì‹œ ì‘ì—… (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)
   - âœ… ìµœì¢…ì ìœ¼ë¡œ values-dev.yamlì— ë°˜ì˜ë¨

**ê²°ë¡ :** âœ… **ëª¨ë“  X-Ray ì„¤ì •ì´ Helm Chartì— í¬í•¨ë˜ì–´ ìˆìœ¼ë©°, Gitì— ì»¤ë°‹ë˜ì–´ ìˆìŠµë‹ˆë‹¤!**

---

### ğŸ“ **ìµœì¢… í™•ì¸ ì²´í¬ë¦¬ìŠ¤íŠ¸**

#### Helm Chart í™•ì¸:
```bash
# X-Ray DaemonSet í…œí”Œë¦¿ ì¡´ì¬
ls helm-chart/templates/xray-daemonset.yaml
# âœ… ì¡´ì¬

# values-dev.yamlì— X-Ray ì„¤ì • ì¡´ì¬
grep -A5 "xray:" helm-chart/values-dev.yaml
# âœ… enabled: true, image, resources ì„¤ì •ë¨

# ì„œë¹„ìŠ¤ í™˜ê²½ë³€ìˆ˜ í™•ì¸
grep "AWS_XRAY_DAEMON_ADDRESS" helm-chart/values-dev.yaml
# âœ… 3ê°œ ì„œë¹„ìŠ¤ ëª¨ë‘ ì„¤ì •ë¨
```

#### ì½”ë“œ í™•ì¸:
```bash
# ëª¨ë“  ì„œë¹„ìŠ¤ì— XRayConfig.java ì¡´ì¬
find backend -name "XRayConfig.java"
# âœ… approval-request, approval-processing, notification ëª¨ë‘ ì¡´ì¬

# pom.xmlì— X-Ray SDK ì¡´ì¬
grep "aws-xray-recorder-sdk-spring" backend/*/pom.xml
# âœ… 3ê°œ ì„œë¹„ìŠ¤ ëª¨ë‘ ì¡´ì¬
```

#### Git í™•ì¸:
```bash
# ëª¨ë“  ë³€ê²½ì‚¬í•­ ì»¤ë°‹ë¨
git log --oneline -5
# bc95b68 feat: Complete X-Ray tracing for all services
# 9bdb6d8 feat: Add X-Ray tracing with logging for debugging
# 73342b6 fix: Initialize X-Ray recorder properly for Spring Boot 3.x
# âœ… ëª¨ë“  X-Ray ì‘ì—… ì»¤ë°‹ë¨
```

---

### ğŸ“ **ë©´ì ‘ ì–´í•„ í¬ì¸íŠ¸**

**Q: X-Rayë¥¼ ì–´ë–»ê²Œ êµ¬í˜„í–ˆë‚˜ìš”?**

**A:** "4ë‹¨ê³„ë¡œ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤. ì²«ì§¸, Spring Bootì— X-Ray SDKë¥¼ ì¶”ê°€í•˜ê³  XRayConfigë¡œ Filterë¥¼ ë“±ë¡í–ˆìŠµë‹ˆë‹¤. ë‘˜ì§¸, Helm Chartì— X-Ray DaemonSetì„ ì¶”ê°€í•˜ì—¬ ê° Nodeì—ì„œ íŠ¸ë ˆì´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ë„ë¡ í–ˆìŠµë‹ˆë‹¤. ì…‹ì§¸, EKS Node Roleì— XRay ê¶Œí•œì„ ë¶€ì—¬í•˜ì—¬ AWS X-Rayë¡œ ë°ì´í„°ë¥¼ ì „ì†¡í•  ìˆ˜ ìˆê²Œ í–ˆìŠµë‹ˆë‹¤. ë„·ì§¸, í™˜ê²½ë³€ìˆ˜ë¡œ Daemon ì£¼ì†Œë¥¼ ì„¤ì •í•˜ì—¬ ì„œë¹„ìŠ¤ê°€ íŠ¸ë ˆì´ìŠ¤ë¥¼ ì „ì†¡í•˜ë„ë¡ í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ Service Mapì—ì„œ ì„œë¹„ìŠ¤ ê°„ í˜¸ì¶œ íë¦„ê³¼ ì‘ë‹µ ì‹œê°„ì„ ì‹œê°í™”í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤."

**Q: CloudWatch Logsì™€ X-Rayì˜ ì°¨ì´ëŠ”?**

**A:** "CloudWatch LogsëŠ” 'ë¬´ì—‡ì´' ì˜ëª»ë˜ì—ˆëŠ”ì§€ íŒŒì•…í•˜ëŠ” ë„êµ¬ì´ê³ , X-RayëŠ” 'ì–´ë””ê°€' ëŠë¦°ì§€ íŒŒì•…í•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤. CloudWatch Logsë¡œ ERROR ë¡œê·¸ë¥¼ ê²€ìƒ‰í•˜ì—¬ ë¬¸ì œë¥¼ ì°¾ê³ , X-Ray Service Mapìœ¼ë¡œ ë³‘ëª© ì§€ì ì„ ì°¾ì•„ ì„±ëŠ¥ì„ ìµœì í™”í•©ë‹ˆë‹¤. ë‘ ë„êµ¬ë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ ì™„ì „í•œ ëª¨ë‹ˆí„°ë§ ì²´ê³„ë¥¼ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤."

---

**"X-Ray ì™„ë²½ êµ¬í˜„ ì™„ë£Œ! ì´ì œ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ íë¦„ì„ í•œëˆˆì— ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤!"** ğŸ‰
